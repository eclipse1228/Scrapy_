{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-27T07:48:18.236119Z",
     "start_time": "2024-07-27T07:48:18.229685Z"
    }
   },
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class Content:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.url = ''\n",
    "        self.title = ''\n",
    "        self.body = ''\n",
    "    \n",
    "    def print(self):\n",
    "        print('URL: {}'.format(self.url))\n",
    "        print('TITLE: {}'.format(self.title))\n",
    "        print('BODY:\\n{}'.format(self.body))\n",
    "        \n",
    "\n",
    "class Website:\n",
    "    \n",
    "    def __init__(self, name, url, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag\n",
    "\n",
    "class Crawler:\n",
    "    \n",
    "    def getPage(self, url):\n",
    "        try:\n",
    "            req = requests.get(url)\n",
    "        except requests.exceptions.RequestException:\n",
    "            return None\n",
    "        return BeautifulSoup(req.text, 'html.parser')\n",
    "    \n",
    "    def safeGet(self,pageObj,selector):\n",
    "        # 콘텐츠 문자열 추출하는 함수 , (없으면 빈문자열)\n",
    "        selectedElems = pageObj.select(selector)\n",
    "        if selectedElems is not None and len(selectedElems) > 0:\n",
    "            return '\\n'.join([elem.get_text() for elem in selectedElems])\n",
    "        return ''\n",
    "    \n",
    "    def parse(self, site,url):\n",
    "        # Url 을 받아 콘텐츠를 추출한다.\n",
    "        bs = self.getPage(url)\n",
    "        if bs is not None:\n",
    "            title = self.safeGet(bs,site.titleTag)\n",
    "            body = self.safeGet(bs, site.bodyTag)\n",
    "            if title != '' and body != '':\n",
    "                content = Content(url,title,body)\n",
    "                content.print()\n",
    "            \n",
    "\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "71ca95f0fb9d8feb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T07:48:42.398153Z",
     "start_time": "2024-07-27T07:48:40.040651Z"
    }
   },
   "source": [
    "crawler = Crawler()\n",
    "\n",
    "siteData = [\n",
    "    ['O\\'Reilly Media','http://oreilly.com','h1','section#product-description'],\n",
    "    ['Reuters','http://reuters.com','h1','div.StandardArticleBody_body_1gnLA'],\n",
    "    ['Brookings','http://www.brookings.edu','h1','div.post-body'],\n",
    "    ['New York Times','http://nytimes.com','h1','p.story-content']\n",
    "]\n",
    "\n",
    "websites = []\n",
    "\n",
    "urls = ['http://oreilly.com','http://reuters.com','http://www.brookings.edu','http://nytimes.com']\n",
    "\n",
    "for row in siteData: \n",
    "    websites.append(Website(row[0],row[1],row[2],row[3]))\n",
    "    \n",
    "crawler.parse(websites[0],urls[0])\n",
    "crawler.parse(websites[1],urls[1])\n",
    "crawler.parse(websites[2],urls[2])\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 모델링 방법 1: 검색을 통한 사이트 크롤링 모델 ",
   "id": "12896dcd57821272"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:22:12.840846Z",
     "start_time": "2024-07-27T08:22:03.661293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Content:\n",
    "    \n",
    "    \n",
    "    def __init__(self,topic,url,title,body):\n",
    "        self.topic = topic\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "        \n",
    "    def print(self):\n",
    "        print('New article found for topic: {}'.format(self.topic))\n",
    "        print('URL: {}'.format(self.url))\n",
    "        print('TITLE: {}'.format(self.title))\n",
    "        print('BODY:\\n{}'.format(self.body))\n",
    "        \n",
    "class Website:\n",
    "    # website 구조 저장 클래스\n",
    "    def __init__(self,name,url,searchUrl,resultLListing,reasultUrl,titleTag,bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.searchUrl = searchUrl\n",
    "        self.resultListing = resultLListing\n",
    "        self.resultUrl = reasultUrl\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag\n",
    "    \n",
    "class Crawler:\n",
    "    \n",
    "    def getPage(self,url):\n",
    "        try:\n",
    "            req = requests.get(url)\n",
    "        except requests.exceptions.RequestException:\n",
    "            return None\n",
    "        return BeautifulSoup(req.text,'html.parser')\n",
    "    \n",
    "    def safeGet(self,pageObj,selector):\n",
    "        childObj = pageObj.select(selector)\n",
    "        if childObj is not None and len(childObj) > 0:\n",
    "            return childObj[0].get_text()\n",
    "        return ''\n",
    "\n",
    "    def search(self,topic,site):\n",
    "        #검색어로 주어진 웹사이트를 검색해 결과 페이지를 모두 기록한다. \n",
    "        bs = self.getPage(site.searchUrl + topic)    \n",
    "        searchResults = bs.select(site.resultListing)\n",
    "        for result in searchResults:\n",
    "            url = result.select(site.resultUrl)[0].attrs['href']\n",
    "            # 상대 Url 인지 절대 Url 인지 확인\n",
    "            if(site.absoluteUrl):\n",
    "                bs = self.getPage(url)\n",
    "            else:\n",
    "                bs = self.getPage(site.url + url)\n",
    "            if bs in None:\n",
    "                print('Something was wrong with that page or URL. Skipping!')\n",
    "                return\n",
    "            \n",
    "            title = self.safeGet(bs,site.titleTag)\n",
    "            body = self.safeGet(bs, site.bodyTag)\n",
    "            if title != '' and body != '':\n",
    "                content = Content(topic,url,title,body)\n",
    "                content.print()\n",
    "\n",
    "crawler = Crawler()\n",
    "\n",
    "siteData = [['O\\'Reilly Media','http://oreilly.com','https://ssearch.oreilly.com/?q=','article.product-result','p.title a','h1','section#product-description'],\n",
    "            ['Reuters','http://reuters.com','http://www.reuters.com/search/news?blob=','div.search-result-content','h3.search-result-title a','h1','div.StandardArticleBody_body_1gnLA'],\n",
    "            ['Brookings','http://www.brookings.edu','https://www.brookings.edu/search/?s=','div.list-content article','h4.title a','h1','div.post-body'],\n",
    "            ['New York Times','http://nytimes.com','https://www.nytimes.com/search?query=','div.SearchResults-main','a.css-11rzbny','h4','p.story-content']]\n",
    "\n",
    "sites = []\n",
    "\n",
    "for row in siteData:\n",
    "    sites.append(Website(row[0],row[1],row[2],row[3],row[4],row[5],row[6]))\n",
    "    \n",
    "\n",
    "topics = ['python','data science']\n",
    "for topic in topics:\n",
    "    print('GETTING INFO ABOUT: ' + topic)\n",
    "    for targetSite in sites:\n",
    "        crawler.search(topic,targetSite)\n"
   ],
   "id": "aa69b77cb72f759",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GETTING INFO ABOUT: python\n",
      "GETTING INFO ABOUT: data science\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "서버 부하를 줄이기 위해서 topics 를 돌면서 각 사이트에 대해 검색을 하는 방법이다. ( 만약, 토픽을 하나의 site에서 다 검색했으면, 부하가 증가했겠지만, 토픽별로 사이트를 순회했음으로\n",
    "부하가 줄었다)\n"
   ],
   "id": "29af348ca6842df7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. 링클르 통한 사이트 크롤링\n",
   "id": "7db0fdb0041e562f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3b2a120f4eeb40cb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
